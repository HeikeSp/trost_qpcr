---
title: "Prediction Model using qPCR data (TROST field)"
author: "Heike Sprenger"
date: "Thursday, March 24, 2016"
output:
  html_document:
    highlight: tango
    number_section: yes
    theme: cerulean
    toc: yes
    toc_depth: 4
---


# Set working directory  
```{r set working directory}
getwd()
#setwd("D:/work/repos/trost_qpcr/")
```


# Load workspace, packages and scripts
```{r load workspace, message=FALSE}
# load packages
library(knitr)
library(reshape)
library(pander)
library(pcaMethods)
library(ggplot2)
library(corrplot)
library(gplots)
library(RColorBrewer)

library(plyr)
library(rpart)
library(randomForest)
library(partykit)
library(party)
library(caret) 
library(pls)
library(BioMark)
library(varSelRF)

# set options for pander
panderOptions('table.split.table', 200)

# set options for knitr
opts_chunk$set(fig.width=5, fig.height=5, cache=FALSE, highlight = TRUE, fig.show="asis")
opts_knit$set(root.dir = '../')

# load workspace
#load("qpcr_model_field.RData")
```


# Load Data
```{r load data}
tolerance <- read.table("../trost_phenotypes/output/tolerance_phenotyper_metadata_2sub.txt", 
                        sep = "\t", header = TRUE)

# relevel tolerance factors
tolerance$tol_cat2_fve <- factor(tolerance$tol_cat2_fve, levels = c("low", "high"))
tolerance$tol_cat3_fve <- factor(tolerance$tol_cat3_fve, levels = c("low","mid", "high"))


samplelist <- read.table("output/trost/samplelist.txt", header=TRUE, sep="\t")
log_norm_ct <- read.table("output/trost/log_norm_ct.txt", header=TRUE, sep="\t")

dim(log_norm_ct)

# percentage of NAs
sum(is.na(log_norm_ct)) / (nrow(log_norm_ct)*ncol(log_norm_ct)) *100
# 0.56%
```


## Define training data
```{r define training data}
# join metadata
head(tolerance)
head(samplelist)
levels(samplelist$treatment) <- c("control", "drought stress")

# join samplelist
samplelist_joined <- join(samplelist, tolerance, by = "subspecies_id")
samplelist_field <- droplevels(subset(samplelist_joined, samplelist_joined$cultivation == "field"))

# only field data
log_norm_ct_field <- subset(log_norm_ct, samplelist$cultivation=="field")
dim(log_norm_ct_field)

# train/test data
train_data <- droplevels(subset(log_norm_ct_field, samplelist_field$model_set=="train"))
test_data <- droplevels(subset(log_norm_ct_field, samplelist_field$model_set=="test"))

# train/test info
train_info <- droplevels(subset(samplelist_field, samplelist_field$model_set=="train"))
test_info <- droplevels(subset(samplelist_field, samplelist_field$model_set=="test"))
```


## Define control data (and meta data)
```{r define control data}
# data
train_data_control <- droplevels(subset(train_data, train_info$treatment=="control"))
test_data_control <- droplevels(subset(test_data, test_info$treatment=="control"))

# meta data
train_info_control <- droplevels(subset(train_info, train_info$treatment=="control"))
test_info_control <- droplevels(subset(test_info, test_info$treatment=="control"))
```


## Define all data (without 3 cultivars --> model_set=="NA") and subset of all control samples
```{r define all data (control)}
all_data <- droplevels(subset(log_norm_ct_field, !samplelist_field$model_set=="NA"))
all_info <- droplevels(subset (samplelist_field, !samplelist_field$model_set=="NA"))

# all control samples (ohne NA -> 3 Kutlivare)
all_data_control <- droplevels(subset(log_norm_ct_field, samplelist_field$treatment=="control" & !samplelist_field$model_set=="NA"))
all_info_control <- droplevels(subset (samplelist_field, samplelist_field$treatment=="control" & !samplelist_field$model_set=="NA"))
```


# Load data without NAs from PCA (rnipals, no scaling)
```{r data without NAs}
log_norm_ct_woNA <- read.table("output/trost/log_norm_ct_none_rnipals_completeObs.txt", header=TRUE, sep="\t")
dim(log_norm_ct_woNA)

# only field data
log_norm_ct_field_woNA <- subset(log_norm_ct_woNA, samplelist$cultivation=="field")
dim(log_norm_ct_field_woNA)

##### train/test #####
train_data_woNA <- subset(log_norm_ct_field_woNA, samplelist_field$model_set=="train")
test_data_woNA <- subset(log_norm_ct_field_woNA, samplelist_field$model_set=="test")

##### control ####
train_data_control_woNA <- subset(train_data_woNA, train_info$treatment=="control")
test_data_control_woNA <- subset(test_data_woNA, test_info$treatment=="control")

##### all ####
all_data_woNA <- subset(log_norm_ct_field_woNA, !samplelist_field$model_set=="NA")

#### all control samples #####
all_data_control_woNA <- subset(log_norm_ct_field_woNA, samplelist_field$treatment=="control" & !samplelist_field$model_set=="NA")
```


# Regression tree: rpart package
## Regression tree: train data
```{r rpart reg train}
class(train_data)
input_train <- data.frame(train_data, "tol" = train_info$mdrym_fve)
class(input_train)

tree_train <- rpart(tol~., data=input_train)
printcp(tree_train)
plot(tree_train)
text(tree_train)

tree_train$cptable[which.min(tree_train$cptable[,"xerror"]),"CP"]
tree_train_pruned <- prune(tree_train, cp=0.01)

plot(tree_train_pruned)
text(tree_train_pruned)

# as party 
plot(as.party(tree_train_pruned))

pdf("figures/model/field_tree_train_pruned.pdf", width=13, height=8)
plot(as.party(prune(tree_train, cp=0.16))) # 1 node
plot(as.party(prune(tree_train, cp=0.13))) # 2 nodes
plot(as.party(prune(tree_train, cp=0.12))) # 3 nodes
plot(as.party(prune(tree_train, cp=0.10))) # 4 nodes
plot(as.party(prune(tree_train, cp=0.06))) # 5 nodes
plot(as.party(prune(tree_train, cp=0.05))) # 6 nodes
plot(as.party(prune(tree_train, cp=0.04))) # 7 nodes
plot(as.party(prune(tree_train, cp=0.0161))) # 8 nodes
plot(as.party(prune(tree_train, cp=0.0160))) # 9 nodes
plot(as.party(prune(tree_train, cp=0.012))) # 10 nodes
dev.off()

###############
# predict
plot(test_info$mdrym_fve, predict(tree_train, as.data.frame(test_data), type="vector"))
sqrt(mean((test_info$mdrym_fve - predict(tree_train, as.data.frame(test_data), type="vector"))^2))
```


## Regression tree: only control field train data
```{r rpart reg train control}
class(train_data_control)
input_train_control <- data.frame(train_data_control, "tol"=train_info_control$mdrym_fve)
class(input_train_control)

tree_train_control <- rpart(tol~., data=input_train_control)
printcp(tree_train_control)
plot(tree_train_control)
text(tree_train_control)

tree_train_control$cptable[which.min(tree_train_control$cptable[,"xerror"]),"CP"]
tree_train_control_pruned <- prune(tree_train_control, cp=0.03831349)

plot(tree_train_control_pruned)
text(tree_train_control_pruned)

# as party 
plot(as.party(tree_train_control_pruned))

pdf("figures/model/field_tree_train_control_pruned.pdf", width=13, height=8)
plot(as.party(prune(tree_train_control, cp=0.25))) # 1 node
plot(as.party(prune(tree_train_control, cp=0.15))) # 2 nodes
plot(as.party(prune(tree_train_control, cp=0.10))) # 3 nodes
plot(as.party(prune(tree_train_control, cp=0.05))) # 4 nodes
plot(as.party(prune(tree_train_control, cp=0.02))) # 5 nodes
plot(as.party(prune(tree_train_control, cp=0.01))) # 6 nodes
dev.off()

###############
# predict
plot(test_info_control$mdrym_fve, predict(tree_train_control, as.data.frame(test_data_control), type="vector"))
sqrt(mean((test_info_control$mdrym_fve - predict(tree_train_control, as.data.frame(test_data_control), type="vector"))^2))
```


## Regression tree: all data
```{r rpart reg all}
input_all <- data.frame(all_data, "tol"=all_info$mdrym_fve)
sum(is.na(all_data))
# 76
dim(all_data)

tree_all <- rpart(tol~., data=input_all)
printcp(tree_all)
plot(tree_all)
text(tree_all)

tree_all$cptable[which.min(tree_all$cptable[,"xerror"]),"CP"]
tree_all_pruned <- prune(tree_all, cp=0.01)

plot(tree_all_pruned)
text(tree_all_pruned)

# as party 
plot(as.party(tree_all_pruned))

pdf("figures/model/field_tree_all_pruned.pdf", width=13, height=8)
plot(as.party(prune(tree_all, cp=0.16))) # 1 node
plot(as.party(prune(tree_all, cp=0.13))) # 2 nodes
plot(as.party(prune(tree_all, cp=0.10))) # 3 nodes
plot(as.party(prune(tree_all, cp=0.09))) # 4 nodes
plot(as.party(prune(tree_all, cp=0.05))) # 6 nodes
plot(as.party(prune(tree_all, cp=0.02))) # 7 nodes
plot(as.party(prune(tree_all, cp=0.016))) # 8 nodes
plot(as.party(prune(tree_all, cp=0.0122))) # 9 nodes
plot(as.party(prune(tree_all, cp=0.01))) # 10 nodes
dev.off()

###############
# predict
plot(test_info$mdrym_fve, predict(tree_all, as.data.frame(test_data), type="vector"))
sqrt(mean((test_info$mdrym_fve - predict(tree_all, as.data.frame(test_data), type="vector"))^2))

# for all data
plot(all_info$mdrym_fve, predict(tree_all, as.data.frame(all_data), type="vector"))
sqrt(mean((all_info$mdrym_fve - predict(tree_all, as.data.frame(all_data), type="vector"))^2))
```


## Regression tree: all data control
```{r rpart reg all control}
input_all_control <- data.frame(all_data_control, "tol"=all_info_control$mdrym_fve)

tree_all_control <- rpart(tol~., data=input_all_control)
printcp(tree_all_control)
plot(tree_all_control)
text(tree_all_control)

tree_all_control$cptable[which.min(tree_all_control$cptable[,"xerror"]),"CP"]
tree_all_control_pruned <- prune(tree_all_control, cp=0.01)

plot(tree_all_control_pruned)
text(tree_all_control_pruned)

# as party 
plot(as.party(tree_all_control_pruned))

pdf("figures/model/field_tree_all_control_pruned.pdf", width=13, height=8)
#plot(as.party(prune(tree_all_control, cp=0.21))) # 1 node
plot(as.party(prune(tree_all_control, cp=0.15))) # 2 nodes
plot(as.party(prune(tree_all_control, cp=0.10))) # 3 nodes
plot(as.party(prune(tree_all_control, cp=0.06))) # 4 nodes
plot(as.party(prune(tree_all_control, cp=0.05))) # 5 nodes
plot(as.party(prune(tree_all_control, cp=0.015))) # 6 nodes
plot(as.party(prune(tree_all_control, cp=0.01))) # 7 nodes
dev.off()

###############
# predict
plot(test_info_control$mdrym_fve, predict(tree_all_control, as.data.frame(test_data_control), type="vector"))
sqrt(mean((test_info_control$mdrym_fve - predict(tree_all_control, as.data.frame(test_data_control), type="vector"))^2))

# for all data
plot(all_info_control$mdrym_fve, predict(tree_all_control, as.data.frame(all_data_control), type="vector"))
sqrt(mean((all_info_control$mdrym_fve - predict(tree_all_control, as.data.frame(all_data_control), type="vector"))^2))
```


# Classification tree: rpart package
## Classification tree: train data, 2 tolerance classes
```{r rpart cat2 train}
input_cat2_train <- data.frame(train_data, "tol"=train_info$tol_cat2_fve)

tree_cat2_train <- rpart(tol~., data=input_cat2_train)
printcp(tree_cat2_train)
plot(tree_cat2_train)
text(tree_cat2_train)

tree_cat2_train$cptable[which.min(tree_cat2_train$cptable[,"xerror"]),"CP"]
tree_cat2_train_pruned <- prune(tree_cat2_train, cp=0.08064516)

plot(tree_cat2_train_pruned)
text(tree_cat2_train_pruned)

# as party 
plot(as.party(tree_cat2_train_pruned))

pdf("figures/model/field_tree_cat2_train_pruned.pdf", width=13, height=8)
plot(as.party(prune(tree_cat2_train, cp=0.2))) # 1 node
plot(as.party(prune(tree_cat2_train, cp=0.09))) # 2 nodes
plot(as.party(prune(tree_cat2_train, cp=0.05))) # 3 nodes
plot(as.party(prune(tree_cat2_train, cp=0.02))) # 4 nodes
dev.off()

###############
# predict
table(train_info$tol_cat2_fve, predict(tree_cat2_train, as.data.frame(train_data), type="class"))
table(test_info$tol_cat2_fve, predict(tree_cat2_train, as.data.frame(test_data), type="class"))
```


## Classification tree: train data, 3 tolerance classes
```{r rpart cat3 train}
input_cat3_train <- data.frame(train_data, "tol"=train_info$tol_cat3_fve)
levels(input_cat3_train$tol)

tree_cat3_train <- rpart(tol~., data=input_cat3_train)
printcp(tree_cat3_train)
plot(tree_cat3_train)
text(tree_cat3_train)

tree_cat3_train$cptable[which.min(tree_cat3_train$cptable[,"xerror"]),"CP"]
tree_cat3_train_pruned <- prune(tree_cat3_train, cp=0.09782609)

plot(tree_cat3_train_pruned)
text(tree_cat3_train_pruned)

# as party 
plot(as.party(tree_cat3_train_pruned))

pdf("figures/model/field_tree_cat3_train_pruned.pdf", width=13, height=8)
plot(as.party(prune(tree_cat3_train, cp=0.20))) # 1 node
plot(as.party(prune(tree_cat3_train, cp=0.10))) # 2 nodes
plot(as.party(prune(tree_cat3_train, cp=0.08))) # 3 nodes
plot(as.party(prune(tree_cat3_train, cp=0.05))) # 7 nodes
dev.off()

###############
# predict
table(train_info$tol_cat3_fve, predict(tree_cat3_train, as.data.frame(train_data), type="class"))
table(test_info$tol_cat3_fve, predict(tree_cat3_train, as.data.frame(test_data), type="class"))
```


## Classification tree: all data, 3 tolerance classes
```{r rpart cat3 all}
input_cat3_all <- data.frame(all_data, "tol"=all_info$tol_cat3_fve)
levels(input_cat3_all$tol)

tree_cat3_all <- rpart(tol~., data=input_cat3_all)
printcp(tree_cat3_all)
plot(tree_cat3_all)
text(tree_cat3_all)

tree_cat3_all$cptable[which.min(tree_cat3_all$cptable[,"xerror"]),"CP"]
# tree_cat3_all_pruned <- prune(tree_cat3_all, cp=0.01)
tree_cat3_all_pruned <- prune(tree_cat3_all, cp=0.05)

plot(tree_cat3_all_pruned)
text(tree_cat3_all_pruned)

# as party 
plot(as.party(tree_cat3_all_pruned))

pdf("figures/model/field_tree_cat3_all_pruned.pdf", width=13, height=8)
plot(as.party(prune(tree_cat3_all, cp=0.20))) # 1 node
plot(as.party(prune(tree_cat3_all, cp=0.13))) # 2 nodes
plot(as.party(prune(tree_cat3_all, cp=0.08))) # 3 nodes
plot(as.party(prune(tree_cat3_all, cp=0.05))) # 7 nodes
dev.off()

###############
# predict
table(all_info$tol_cat3_fve, predict(tree_cat3_all, as.data.frame(all_data), type="class"))
table(test_info$tol_cat3_fve, predict(tree_cat3_all, as.data.frame(test_data), type="class"))
```


# Random forest regression
## Random forest regression: train data
```{r rf reg train}
input_train_woNA <- data.frame(train_data_woNA, "tol"=train_info$mdrym_fve)
class(input_train_woNA)

set.seed(1)
rf_train <- randomForest(tol~. , data=input_train_woNA, ntree=1000)
print(rf_train)

#importance(rf_train)
varImpPlot(rf_train)

plot(train_info$mdrym_fve, rf_train$predicted)
abline(0,1)

plot(test_info$mdrym_fve, predict(rf_train, test_data_woNA))
sqrt(mean((test_info$mdrym_fve - predict(rf_train, as.data.frame(test_data_woNA)))^2))
```


### Cross-Validation
```{r rf reg train CV}
set.seed(3)
rf_train_cv <- rfcv(train_data_woNA, train_info$mdrym_fve, step=0.8)
rf_train_cv$n.var
with(rf_train_cv, plot(n.var, error.cv, log="x", type="o", lwd=2))
```


## Random forest regression: train control data
```{r rf reg train control}
input_train_control_woNA <- data.frame(train_data_control_woNA, "tol"=train_info_control$mdrym_fve)
class(input_train_control_woNA)

set.seed(1)
rf_train_control <- randomForest(tol~. , data=input_train_control_woNA, ntree=1000)
print(rf_train_control)

#importance(rf_train_control)
varImpPlot(rf_train_control)

plot(train_info_control$mdrym_fve, rf_train_control$predicted)
abline(0,1)

plot(test_info_control$mdrym_fve, predict(rf_train_control, test_data_control_woNA))
sqrt(mean((test_info_control$mdrym_fve - predict(rf_train_control, as.data.frame(test_data_control_woNA)))^2))
```


## Random forest regression: all data
```{r rf reg all}
input_all_woNA <- data.frame(all_data_woNA, "tol"=all_info$mdrym_fve)

set.seed(1)
rf_all <- randomForest(tol~. , data=input_all_woNA, ntree=1000)
print(rf_all)

#importance(rf_all)
varImpPlot(rf_all)

plot(all_info$mdrym_fve, rf_all$predicted)
abline(0,1)

plot(test_info$mdrym_fve, predict(rf_all, test_data_woNA))
sqrt(mean((test_info$mdrym_fve - predict(rf_all, as.data.frame(test_data_woNA)))^2))
```


### Cross-Validation
```{r rf reg all CV}
set.seed(3)
rf_all_cv <- rfcv(all_data_woNA, all_info$mdrym_fve, step=0.8)
rf_all_cv$n.var
with(rf_all_cv, plot(n.var, error.cv, log="x", type="o", lwd=2))
```


## Random forest regression: ALL control data
```{r rf reg all control}
input_all_control_woNA <- data.frame(all_data_control_woNA, "tol"=all_info_control$mdrym_fve)

set.seed(1)
rf_all_control <- randomForest(tol~. , data=input_all_control_woNA, ntree=1000)
print(rf_all_control)

#importance(rf_all_control)
varImpPlot(rf_all_control)

plot(all_info_control$mdrym_fve, rf_all_control$predicted)
abline(0,1)

plot(test_info_control$mdrym_fve, predict(rf_all_control, test_data_control_woNA))
sqrt(mean((test_info_control$mdrym_fve - predict(rf_all_control, as.data.frame(test_data_control_woNA)))^2))
```


# Random forest classification
## Random forest classification: train data with 2 classes
```{r rf cat2 train}
input_cat2_train_woNA <- data.frame(train_data_woNA, "tol"=train_info$tol_cat2_fve)
levels(input_cat2_train_woNA$tol)

set.seed(1)
rf_cat2_train <- randomForest(tol~. , data=input_cat2_train_woNA, ntree=1000)
print(rf_cat2_train)

write.table(importance(rf_cat2_train), "output/model/importance_field_rf_cat2_train.txt", sep="\t")
varImpPlot(rf_cat2_train)

table(predict(rf_cat2_train, test_data_woNA), test_info$tol_cat2_fve)
confusionMatrix(table(predict(rf_cat2_train, test_data_woNA), test_info$tol_cat2_fve))
```


## Random forest classification: train data with 3 classes
```{r rf cat3 train}
input_cat3_train_woNA <- data.frame(train_data_woNA, "tol"=train_info$tol_cat3_fve)
levels(input_cat3_train_woNA$tol)

set.seed(1)
rf_cat3_train <- randomForest(tol~. , data=input_cat3_train_woNA, ntree=1000)
print(rf_cat3_train)

write.table(importance(rf_cat3_train), "output/model/importance_field_rf_cat3_train.txt", sep="\t")
varImpPlot(rf_cat3_train)

table(predict(rf_cat3_train, test_data_woNA), test_info$tol_cat3_fve)
confusionMatrix(table(predict(rf_cat3_train, test_data_woNA), test_info$tol_cat3_fve))
```


### Cross-Validation
**rfcv**: This function shows the cross-validated prediction performance of models 
with sequentially reduced number of predictors 
(ranked by variable importance) via a nested cross-validation procedure. 

```{r rf cat3 train CV}
set.seed(1)
rf_cat3_train_cv <- rfcv(train_data_woNA, train_info$tol_cat3_fve, step=0.8)
rf_cat3_train_cv$n.var
length(rf_cat3_train_cv$n.var)
with(rf_cat3_train_cv, plot(n.var, error.cv, log="x", type="o", lwd=2))


#pdf("../../../../Doktorarbeit/figures/model_field_transcripts_rfcv_cat3_train.pdf", width=6, height=6)
pdf("figures/thesis/model_field_transcripts_rfcv_cat3_train.pdf", width=6, height=6)
par(mar=c(4.5, 4.5, 0.5, 0.6))
with(rf_cat3_train_cv, plot(n.var, error.cv, log="x", type="o", lwd=2, cex.lab=1.5, cex.axis=1.2,
                            xlab="number of predictors", ylab="5-fold cross-validation error"))
dev.off()
```


### Variable Selection (vs)
Using the OOB error as minimization criterion, carry out variable elimination from random forest, 
by successively eliminating the least important variables (with importance as returned from random forest).
```{r rf cat3 train VarSel}
set.seed(1)
rf_cat3_train_vs <- varSelRF(train_data_woNA, train_info$tol_cat3_fve, ntree = 500, ntreeIterat = 300, vars.drop.frac = 0.2, c.sd=1)
#rf_cat3_train_vs <- varSelRF(train_data_woNA, train_info$tol_cat3_fve, ntree = 5000, ntreeIterat = 1000, vars.drop.frac = 0.2, c.sd=1)
rf_cat3_train_vs
# 18
write.table(rf_cat3_train_vs$selected.vars, "output/model/field_rf_cat3_train_selected_vars.txt", sep="\t")
rf_cat3_train_vs$selected.vars
plot(rf_cat3_train_vs, which=1)
plot(rf_cat3_train_vs, which=2)

# indices of selected variables
vs_idx <- which(colnames(train_data_woNA) %in% rf_cat3_train_vs$selected.vars)
```


## Random forest classification: selected variables from field data with 3 classes
```{r rf cat3 train selected variables, fig.width=12}
train_data_woNA_vs <- train_data_woNA[,vs_idx]
test_data_woNA_vs <- test_data_woNA[,vs_idx]

input_train_cat3_woNA_vs <- data.frame(train_data_woNA_vs, "tol"=train_info$tol_cat3_fve)
levels(input_train_cat3_woNA_vs$tol)

set.seed(1)
rf_cat3_train_reduced <- randomForest(tol~. , data=input_train_cat3_woNA_vs, ntree=1000)
print(rf_cat3_train_reduced)

write.table(importance(rf_cat3_train_reduced), "output/model/importance_rf_cat3_train_reduced.txt", sep="\t")
par(mfrow=c(1,2))
varImpPlot(rf_cat3_train_reduced)
varImpPlot(rf_cat3_train)
par(mfrow=c(1,1))

table(predict(rf_cat3_train_reduced, test_data_woNA_vs), test_info$tol_cat3_fve)
confusionMatrix(table(predict(rf_cat3_train_reduced, test_data_woNA_vs), test_info$tol_cat3_fve))
```


## Random forest classification: train control data with 3 classes
```{r rf cat3 train control}
input_cat3_train_control_woNA <- data.frame(train_data_control_woNA, "tol"=train_info_control$tol_cat3_fve)
levels(input_cat3_train_control_woNA$tol)

set.seed(1)
rf_cat3_train_control <- randomForest(tol~. , data=input_cat3_train_control_woNA, ntree=1000)
print(rf_cat3_train_control)

write.table(importance(rf_cat3_train_control), "output/model/importance_field_rf_cat3_train_control.txt", sep="\t")
varImpPlot(rf_cat3_train_control)

table(predict(rf_cat3_train_control, test_data_control_woNA), test_info_control$tol_cat3_fve)
confusionMatrix(table(predict(rf_cat3_train_control, test_data_control_woNA), test_info_control$tol_cat3_fve))
```


## Random forest classification: all data with 3 classes
```{r rf cat3 all}
input_cat3_all_woNA <- data.frame(all_data_woNA, "tol"=all_info$tol_cat3_fve)
levels(input_cat3_all_woNA$tol)

set.seed(1)
rf_cat3_all <- randomForest(tol~. , data=input_cat3_all_woNA, ntree=1000)
print(rf_cat3_all)

write.table(importance(rf_cat3_all), "output/model/importance_field_rf_cat3_all.txt", sep="\t")
varImpPlot(rf_cat3_all)

table(all_info$tol_cat3_fve, rf_cat3_all$predicted)
```


### Cross-Validation
```{r rf cat3 all CV}
set.seed(13)
rf_cat3_all_cv <- rfcv(all_data_woNA, all_info$tol_cat3_fve, step=0.8)
rf_cat3_all_cv$n.var
with(rf_cat3_all_cv, plot(n.var, error.cv, log="x", type="o", lwd=2))
```


### Variable Selection (vs)
```{r rf cat3 all VarSel}
set.seed(1)
rf_cat3_all_vs <- varSelRF(all_data_woNA, all_info$tol_cat3_fve, ntree = 500, ntreeIterat = 300, vars.drop.frac = 0.2, c.sd=1)
rf_cat3_all_vs
# 29
write.table(rf_cat3_all_vs$selected.vars, "output/model/field_rf_cat3_all_selected_vars.txt", sep="\t")
rf_cat3_all_vs$selected.vars
plot(rf_cat3_all_vs, which=1)
plot(rf_cat3_all_vs, which=2)

# indices of selected variables
rf_cat3_all_vs_idx <- which(colnames(all_data_woNA) %in% rf_cat3_all_vs$selected.vars)
```


## Random forest classification: selected variables from all data with 3 classes
```{r rf cat3 all selected variables, fig.width=12}
all_data_woNA_vs <- all_data_woNA[,rf_cat3_all_vs_idx]

input_all_cat3_woNA_vs <- data.frame(all_data_woNA_vs, "tol"=all_info$tol_cat3_fve)
levels(input_all_cat3_woNA_vs$tol)

set.seed(1)
rf_cat3_all_reduced <- randomForest(tol~. , data=input_all_cat3_woNA_vs, ntree=1000)
print(rf_cat3_all_reduced)

write.table(importance(rf_cat3_all_reduced), "output/model/importance_rf_cat3_all_reduced.txt", sep="\t")
par(mfrow=c(1,2))
varImpPlot(rf_cat3_all_reduced)
varImpPlot(rf_cat3_all)
par(mfrow=c(1,1))
```


## Random forest classification: all control data with 3 classes
```{r rf cat3 all control}
input_cat3_all_control_woNA <- data.frame(all_data_control_woNA, "tol"=all_info_control$tol_cat3_fve)
levels(input_cat3_all_control_woNA$tol)

set.seed(1)
rf_cat3_all_control <- randomForest(tol~. , data=input_cat3_all_control_woNA, ntree=1000)
print(rf_cat3_all_control)

write.table(importance(rf_cat3_all_control), "output/model/importance_field_rf_cat3_all_control.txt", sep="\t")
varImpPlot(rf_cat3_all_control)

table(all_info_control$tol_cat3_fve, rf_cat3_all_control$predicted)
```


# Export importance tables
```{r export importance tables}
importance_rf_reg <- data.frame(importance(rf_train), 
                            importance(rf_train_control), 
                            importance(rf_all), 
                            importance(rf_all_control))
colnames(importance_rf_reg) <- c("reg_train", "reg_train_control", "reg_all", "reg_all_control")
head(importance_rf_reg)

importance_rf_cat3 <- data.frame(importance(rf_cat3_train), 
                            importance(rf_cat3_train_control), 
                            importance(rf_cat3_all), 
                            importance(rf_cat3_all_control))
colnames(importance_rf_cat3) <- c("cat3_train", "cat3_train_control", "cat3_all", "cat3_all_control")
head(importance_rf_cat3)

importance_rf <- cbind(importance_rf_reg, importance_rf_cat3)
importance_rf_sort <- importance_rf[order(importance_rf$cat3_train),]
head(importance_rf_sort)
importance_rf_sort2 <- importance_rf[order(importance_rf$cat3_all),]
```


# Plot importance
```{r plot importance}
#pdf("../../../../Doktorarbeit/figures/model_field_transcripts_importance_cat3_train.pdf", width=6, height=6)
pdf("figures/thesis/model_field_transcripts_importance_cat3_train.pdf", width=6, height=6)
par(mar=c(4.5, 18, 0.5, 0.5))
barplot(tail(importance_rf_sort$cat3_train,n=20), horiz = T, names=tail(rownames(importance_rf_sort),n=20), las=2, xaxt="n", cex.axis=1.5, cex.names=1.5)
axis(1, las=1)
title(xlab="Importance", cex.lab=1.5, cex.axis=1.2)
dev.off()

#pdf("../../../../Doktorarbeit/figures/model_field_transcripts_importance_cat3_all.pdf", width=6, height=6)
#par(mar=c(4.5, 19, 0.5, 0.5))
barplot(tail(importance_rf_sort2$cat3_all,n=20), horiz = T, names=tail(rownames(importance_rf_sort2),n=20), las=2, xaxt="n", cex.axis=1.5, cex.names=1.5)
axis(1, las=1)
title(xlab="Importance", cex.lab=1.5, cex.axis=1.2)
#dev.off()


plot(importance_rf_cat3$cat3_train, importance_rf_cat3$cat3_train_control, pch=19, cex=1.5)
plot(importance_rf_cat3$cat3_train, importance_rf_reg$reg_train, pch=19, cex=1.5)
plot(importance_rf_cat3$cat3_train, importance_rf_cat3$cat3_all, pch=19, cex=1.5)

write.table(importance_rf_reg, "output/model/field_importance_rf_reg.txt", sep="\t")
write.table(importance_rf_cat3, "output/model/field_importance_rf_cat3.txt", sep="\t")
```


## Try RF LOO-CV with single cultivars for test set

* Idea: use 30 of 31 cultivars in training and the remaining for test

```{r try LOO-CV}
dim(all_data_woNA)
dim(all_info)

table(all_info$cultivar)
length(levels(all_info$cultivar))

genotype_levels <- levels(all_info$cultivar)

loocv_rf_accuracy <- rep(NA, 31)
names(loocv_rf_accuracy) <- genotype_levels

loocv_lasso_accuracy <- loocv_lasso_err <- loocv_rf_err <- loocv_rf_accuracy


for (i in genotype_levels){
  loocv_training_data <- subset(all_data_woNA, !all_info$cultivar == i)
  loocv_test_data <- subset(all_data_woNA, all_info$cultivar == i)
  
  loocv_training_info <- subset(all_info, !all_info$cultivar == i)
  loocv_test_info <- subset(all_info, all_info$cultivar == i)
  
  loocv_input <- data.frame(loocv_training_data, "tol" = loocv_training_info$tol_cat3_fve, check.names = FALSE)
  loocv_input_reg <- data.frame(loocv_training_data, "tol" = loocv_training_info$mdrym_fve, check.names = FALSE)
  tol <- match("tol", names(loocv_input)) # i is index of tol column

  # RANDOM FOREST
  set.seed(1234)
  loocv_rf <- randomForest(loocv_input[,-tol], loocv_input[,tol], ntree=1000)
  #table(predict(loocv_rf, loocv_test_data), loocv_test_info$tol_cat3_fve)
  
  # save accuracy
  loocv_rf_accuracy[i] <- confusionMatrix(predict(loocv_rf, loocv_test_data), 
                                       loocv_test_info$tol_cat3_fve)$overall[1]
  
  # RF regression
  loocv_rf_reg <- randomForest(loocv_input_reg[,-tol], loocv_input_reg[,tol], ntree=1000)
  predict(loocv_rf_reg, loocv_test_data)
  loocv_rf_err[i] <- sqrt(mean((loocv_test_info$mdrym_fve - predict(loocv_rf_reg, loocv_test_data))^2))
  
  # LASSO
  loocv_lasso <- cv.glmnet(as.matrix(loocv_training_data), 
                           loocv_training_info$tol_cat3_fve, family = "multinomial")
  loocv_lasso_reg <- cv.glmnet(as.matrix(loocv_training_data),
                               loocv_training_info$mdrym_fve)
  
  loocv_lasso_pred <- as.factor(predict(loocv_lasso, new = as.matrix(loocv_test_data), 
                                        s = "lambda.min", type = "class")[,1])
  loocv_lasso_reg_pred <- predict(loocv_lasso_reg, new = as.matrix(loocv_test_data), s = "lambda.min")
  
  loocv_lasso_accuracy[i] <- confusionMatrix(loocv_lasso_pred, 
                                             loocv_test_info$tol_cat3_fve)$overall[1]
  
  loocv_lasso_err[i] <- sqrt(mean((loocv_test_info$mdrym_fve - loocv_lasso_reg_pred)^2))
  
}

loocv_rf_accuracy
hist(loocv_rf_accuracy)

plot(loocv_lasso_accuracy, loocv_rf_accuracy)
plot(loocv_lasso_err, loocv_rf_err)



```


# PLS
## Step 1. PLS (Partial Least Squares) for all variables to compute VIP
```{r pls all}
input_all_woNA <- data.frame(train_data_woNA, "tol"=train_info$mdrym_fve)

pls_all <- plsr(tol ~ ., data=input_all_woNA, validation="LOO", method="oscorespls")

# summary(pls_all)

par(mfrow=c(1,2))
plot(pls_all, "validation", estimate="CV")
par(pty="s")
plot(pls_all, "prediction", ncomp=4)
abline(0,1, col="gray")
par(mfrow=c(1,1))

plot(RMSEP(pls_all), legendpos="topright") # Plot the root mean squared error of prediction (RMSEP) to get the number of components to be used

RMSEP(pls_all)
M.pls <- which.min(RMSEP(pls_all, estimate="adjCV")$val) - 1
# 12 --> 11 comps!
plot(pls_all, "validation", estimate="adjCV")
abline(v=11, col="red")

which.max(R2(pls_all)$val)
plot(R2(pls_all))
# 12 --> 11 comps!
abline(v=11, col="red")

test_data_woNA_2 <- data.frame(test_data_woNA, "tol"=test_info$mdrym_fve)
RMSEP(pls_all, ncomp=1, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=2, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=3, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=4, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=5, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=6, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=7, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=8, newdata=test_data_woNA_2, intercept=FALSE)
RMSEP(pls_all, ncomp=9, newdata=test_data_woNA_2, intercept=FALSE)

# The interpretation of the scores and loadings is similar to PCA: a score indicates how much a particular object contributes to a latent variable, while a loading indicates the contribution of a particular variable. An example of a loading plot is obtained using the code below:
plot(pls_all, "loading", comps = 1:3, legendpos = "top", lty = c(1, 2, 4), col = c(1, 2, 4))

y.est=predict(pls_all, newdata=test_data_woNA_2, ncomp=M.pls)
cor(y.est, test_info$mdrym_fve)
```


## VIP function
```{r VIP function}
VIPjh=function(object, j, h)
{
   if (object$method!="oscorespls")
      stop("Only implemented for orthogonal scores algorithm. Refit with 'method = \"oscorespls\"'")
   if (nrow(object$Yloadings) > 1)
      stop("Only implemented for single-response models")

   b=c(object$Yloadings)[1:h]
   T=object$scores[,1:h, drop=F]
   SS=b^2 * colSums(T^2)
   W=object$loading.weights[,1:h, drop=F]
   Wnorm2=colSums(W^2)
   sqrt(nrow(W) * sum(SS * W[j,]^2 / Wnorm2) / sum(SS))
}
```


## Step 2. Compute VIP-value for all variables and  Step 3. Sort them accordingly

```{r compute VIP values}
p <- ncol(train_data)
VIP=vector("numeric", p)
I.pls=1:p

for(j in 1:p)
   VIP[j]=VIPjh(pls_all, j, M.pls)

# Plot result
pdf("figures/model/field_VIP_Step_2.pdf")
plot(VIP)
dev.off()

VIP_table <- cbind(colnames(train_data), VIP)
head(VIP_table[order(VIP_table[,2], decreasing=T),])

VIPmax=sort(VIP, decreasing=T)
for (i in 1:p)
I.pls[i]=which(VIP==VIPmax[i])

# for 1 component:
VIP_1=vector("numeric", p)
for(j in 1:p)
   VIP_1[j]=VIPjh(pls_all, j, 1)
VIP_1_table <- cbind(colnames(train_data), VIP_1)
head(VIP_1_table[order(VIP_1_table[,2], decreasing=T),])

```


# BioMark package
```{r BioMark}
# stability_all <- get.biom(X = train_data_woNA, Y = train_info$mdrym_fve, fmethod=c("pcr", "pls", "vip", "lasso"), type = "stab")
# summary(stability_all)
# selection(stability_all)
# 
# # only two class discrimination possible!
# HC_cat2 <- get.biom(X = train_data_woNA, Y = train_info$tol_cat2_fve, fmethod=c("studentt","pcr", "pls", "vip"), type = "HC")
# summary(HC_cat2)
# selection(HC_cat2)
# 
# selection(HC_cat2)$pls[[1]]
# selection(HC_cat2)$vip[[1]]
# selection(HC_cat2)$pcr[[1]]
# intersect( selection(HC_cat2)$pls[[1]], selection(HC_cat2)$vip[[1]])
# colnames(train_data_control_woNA)
```


# Save workspace and sessionInfo
```{r save workspace}
save.image("qpcr_model_field.RData")
sessionInfo()
```

